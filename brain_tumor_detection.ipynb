{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "brain_tumor_detection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "UUhr10WVNNgJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xxyavb7Pevd8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download a file based on its file ID.\n",
        "download = drive.CreateFile({'id': '16hA5xU-EmCSRAVEkK_xtYQv8FWyvHOl6'})\n",
        "download.GetContentFile('dataset.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kj9HUXhzcZ7-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip dataset.zip\n",
        "!rm dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KI3pAgUkIxyt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Part 1 - Building the CNN\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
        "\n",
        "# Initialising the CNN or classifier\n",
        "classifier = Sequential()\n",
        "\n",
        "# Step 1 - Convolution, pooling!\n",
        "classifier.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(224, 224, 1)))\n",
        "classifier.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=2))\n",
        "classifier.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
        "classifier.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=2))\n",
        "classifier.add(Conv2D(filters=256, kernel_size=2, padding='same', activation='relu'))\n",
        "classifier.add(Conv2D(filters=256, kernel_size=2, padding='same', activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=2))\n",
        "classifier.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='relu'))\n",
        "classifier.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='relu'))\n",
        "classifier.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=2))\n",
        "classifier.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='relu'))\n",
        "classifier.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='relu'))\n",
        "classifier.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=2))\n",
        "classifier.add(BatchNormalization())\n",
        "\n",
        "# Step 3 - Flattening\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Step 4 - Full connection\n",
        "classifier.add(Dense(units = 4096, activation = 'relu'))\n",
        "classifier.add(Dropout(0.5))\n",
        "classifier.add(Dense(units = 4096, activation = 'relu'))\n",
        "classifier.add(Dropout(0.5))\n",
        "classifier.add(Dense(units = 1000, activation = 'relu'))\n",
        "classifier.add(Dropout(0.3))\n",
        "classifier.add(Dense(units = 3, activation = 'sigmoid'))\n",
        "\n",
        "# Compiling the CNN or classifier\n",
        "classifier.compile(optimizer = Adam(lr=3e-06, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Part 2 - Fitting the CNN to the images\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def plotHistory(history):\n",
        "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
        "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
        "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
        "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
        "    \n",
        "    if len(loss_list) == 0:\n",
        "        print('Loss is missing in history')\n",
        "        return \n",
        "    \n",
        "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
        "    \n",
        "    ## Loss\n",
        "    plt.figure(1)\n",
        "    for l in loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
        "    for l in val_loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
        "    \n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    ## Accuracy\n",
        "    plt.figure(2)\n",
        "    for l in acc_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
        "    for l in val_acc_list:    \n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
        "\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()    \n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('dataset/training', target_size = (224, 224),color_mode='grayscale',batch_size = 32,class_mode = 'categorical')\n",
        "\n",
        "validation_set = test_datagen.flow_from_directory('dataset/validation', target_size = (224, 224), color_mode='grayscale', batch_size = 32, class_mode = 'categorical')\n",
        "\n",
        "try:\n",
        "  history = classifier.fit_generator(training_set, steps_per_epoch = 94, callbacks = [EarlyStopping('val_acc', mode='auto', patience=3)], epochs = 50, validation_data = validation_set, validation_steps = 37)\n",
        "  plotHistory(history)\n",
        "finally:\n",
        "  classifier.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0K-KzVB6djsv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Library imports\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "\n",
        "def rgb2gray(gimage):\n",
        "    gimage = 0.2989 * gimage[:,:,0] + 0.5870 * gimage[:,:,1] + 0.1140 * gimage[:,:,2]\n",
        "    gimage = np.expand_dims(gimage, axis=2)\n",
        "    return gimage\n",
        "  \n",
        "correct = 0\n",
        "total_img = 0        \n",
        "tumor_types = ['glioma', 'meningioma', 'pituitary']\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# Detection accuracy\n",
        "for index, tumor in enumerate(tumor_types):\n",
        "        \n",
        "    for i in range (1001, 1201):\n",
        "        name = 'dataset/validation/' + tumor + '/' + tumor + str(i) + '.png'\n",
        "        test_image = image.load_img(name, target_size = (224, 224))\n",
        "        test_image = image.img_to_array(test_image)\n",
        "        test_image = rgb2gray(test_image)\n",
        "        test_image = np.expand_dims(test_image, axis=0)\n",
        "        result = model.predict(test_image)\n",
        "        if result[0][index] == 1:\n",
        "            correct = correct + 1\n",
        "        total_img = total_img + 1\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = float(correct) * 100 / float(total_img)\n",
        "print(str(correct) + \" out of \" + str(total_img) + \" are correctly classified\")\n",
        "print(\"acurracy of the model on this class is \" + str(accuracy) + \"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ybHj87SXApiS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# upload weights file to google drive.\n",
        "upload = drive.CreateFile({'title': 'model.h5'})\n",
        "upload.SetContentFile('model.h5')\n",
        "upload.Upload()\n",
        "print('Uploaded file with ID {}'.format(upload.get('id')))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}